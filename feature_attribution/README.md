# Feature Attribution/Importance


## Papers

[DERIVING EXPLAINABLE DISCRIMINATIVE ATTRIBUTES USING CONFUSION ABOUT COUNTERFACTUAL CLASS](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9747693), ICASSP 2022

[FastSHAP: Real-Time Shapley Value Estimation](https://openreview.net/pdf?id=Zq2G_VTV53T), ICLR 2022

[Explain, Edit, and Understand: Rethinking User Study Design for Evaluating Model Explanations](https://arxiv.org/pdf/2112.09669.pdf), AAAI 2022

[Backdoor Attacks on the DNN Interpretation System](https://arxiv.org/pdf/2011.10698.pdf), AAAI 2022

[Feature Importance Explanations for Temporal Black-Box Models](https://arxiv.org/pdf/2102.11934.pdf), AAAI 2022

[Evaluating Explainable AI on a Multi-Modal Medical Imaging Task: Can Existing Algorithms Fulfill Clinical Requirements?](https://www2.cs.sfu.ca/~hamarneh/ecopy/aaai2022.pdf), AAAI 2022

[Do Feature Attribution Methods Correctly Attribute Features?](https://arxiv.org/pdf/2104.14403.pdf), AAAI 2022

[One Explanation is Not Enough: Structured Attention Graphs for Image Classification](https://openreview.net/pdf?id=k5Kbs9uPGP9), NeurIPS 2021

[On Locality of Local Explanation Models](https://arxiv.org/abs/2106.14648), NeurIPS 2021

[Shapley Residuals: Quantifying the limits of the Shapley value for explanations](https://par.nsf.gov/servlets/purl/10187138), NeurIPS 2021

[The Out-of-Distribution Problem in Explainability and Search Methods for Feature Importance Explanations](https://arxiv.org/abs/2106.00786), NeurIPS 2021

[Reliable Post hoc Explanations: Modeling Uncertainty in Explainability](https://arxiv.org/abs/2008.05030), NeurIPS 2021

[Look at the Variance! Efficient Black-box Explanations with Sobol-based Sensitivity Analysis](https://arxiv.org/abs/2111.04138), NeurIPS 2021

[Do Input Gradients Highlight Discriminative Features?](https://arxiv.org/abs/2102.12781), NeurIPS 2021

[The effectiveness of feature attribution methods and its correlation with automatic evaluation scores](https://arxiv.org/abs/2105.14944), NeurIPS 2021

[From global to local MDI variable importances for random forests and when they are Shapley values](https://arxiv.org/abs/2111.02218), NeurIPS 2021 
[On Guaranteed Optimal Robust Explanations for NLP Models](https://arxiv.org/pdf/2105.03640.pdf), IJCAI 2021

[What does LIME really see in images?](https://arxiv.org/abs/2102.06307), ICML 2021

[Explanations for Monotonic Classifiers](https://arxiv.org/pdf/2106.00154.pdf), ICML 2021

[Explaining Time Series Predictions with Dynamic Masks](https://arxiv.org/pdf/2106.05303.pdf), ICML 2021

[On Explainability of Graph Neural Networks via Subgraph Explorations](http://proceedings.mlr.press/v139/yuan21c.html), ICML 2021

[Generative Causal Explanations for Graph Neural Networks](http://proceedings.mlr.press/v139/lin21d/lin21d.pdf), ICML 2021

[How Interpretable and Trustworthy are GAMs?](https://arxiv.org/pdf/2006.06466.pdf), KDD 2021

[Leveraging Latent Features for Local Explanations](https://arxiv.org/abs/1905.12698), KDD 2021

[S-LIME: Stabilized-LIME for Model Explanation](https://arxiv.org/abs/2106.07875), KDD 2021

[An Experimental Study of Quantitative Evaluations on Saliency Methods](https://arxiv.org/pdf/2012.15616.pdf), KDD 2021

[TimeSHAP: Explaining Recurrent Models through Sequence Perturbations](https://arxiv.org/abs/2012.00073), KDD 2021

[Black-box Explanation of Object Detectors via Saliency Maps](https://openaccess.thecvf.com/content/CVPR2021/papers/Petsiuk_Black-Box_Explanation_of_Object_Detectors_via_Saliency_Maps_CVPR_2021_paper.pdf), CVPR 2021

[Interpreting Super-Resolution Networks with Local Attribution Maps](https://openaccess.thecvf.com/content/CVPR2021/papers/Gu_Interpreting_Super-Resolution_Networks_With_Local_Attribution_Maps_CVPR_2021_paper.pdf), CVPR 2021

[Transformer Interpretability Beyond Attention Visualization](https://openaccess.thecvf.com/content/CVPR2021/papers/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.pdf), CVPR 2021

[A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts](https://openaccess.thecvf.com/content/CVPR2021/papers/Ge_A_Peek_Into_the_Reasoning_of_Neural_Networks_Interpreting_With_CVPR_2021_paper.pdf), CVPR 2021

[Building Reliable Explanations of Unreliable Neural Networks: Locally Smoothing Perspective of Model Interpretation](https://openaccess.thecvf.com/content/CVPR2021/papers/Lim_Building_Reliable_Explanations_of_Unreliable_Neural_Networks_Locally_Smoothing_Perspective_CVPR_2021_paper.pdf), CVPR 2021

[Relevance-CAM: Your Model Already Knows Where to Look](https://openaccess.thecvf.com/content/CVPR2021/papers/Lee_Relevance-CAM_Your_Model_Already_Knows_Where_To_Look_CVPR_2021_paper.pdf), CVPR 2021, [code](https://github.com/mongeoroo/Relevance-CAM)

[An Analysis of LIME for Text Data](https://arxiv.org/pdf/2010.12487.pdf), AISTATS 2021

[Improving KernelSHAP: Practical Shapley Value Estimation Using Linear Regression](https://arxiv.org/pdf/2012.01536.pdf), AISTATS 2021

[If You Like Shapley Then You’ll Love the Core](), AAAI 2021

[Interpreting Deep Neural Networks with Relative Sectional Propagation by Analyzing Comparative Gradients and Hostile Activations](https://arxiv.org/pdf/2012.03434.pdf), AAAI 2021

[Explaining Convolutional Neural Networks through Attribution-Based Input Sampling and Block-Wise Feature Aggregation](https://arxiv.org/pdf/2010.00672.pdf), AAAI 2021

[Explainable Models with Consistent Interpretations](https://www.csee.umbc.edu/~hpirsiav/papers/gc_aaai21.pdf), AAAI 2021

[On the Tractability of SHAP Explanations](https://arxiv.org/abs/2009.08634), AAAI 2021

[Interpreting Multivariate Shapley Interactions in DNNs](https://arxiv.org/abs/2010.05045), AAAI 2021

[Interpreting Deep Neural Networks with Relative Sectional Propagation by Analyzing Comparative Gradients and Hostile Activations](https://arxiv.org/pdf/2012.03434.pdf), AAAI 2021

[Interpreting Graph Neural Networks for NLP With Differentiable Edge Masking](https://openreview.net/forum?id=WznmQa42ZAx), ICLR 2021

[Scaling Symbolic Methods using Gradients for Neural Model Explanation](https://openreview.net/forum?id=V5j-jdoDDP), ICLR 2021

[Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability](https://openreview.net/forum?id=dYeAHXnpWJ4), ICLR 2021

[Shapley explainability on the data manifold](https://openreview.net/forum?id=OPyWRrcjVQw), ICLR 2021

[ICAM: Interpretable Classification via Disentangled Representations and Feature Attribution Mapping](https://arxiv.org/abs/2006.08287), NeurIPS 2020

[What went wrong and when? Instance-wise Feature Importance for Time-series Models](https://arxiv.org/abs/2003.02821), NeurIPS 2020

[How Can I Explain This to You? An Empirical Study of Deep Neural Network Explanation Methods](https://proceedings.neurips.cc/paper/2020/file/2c29d89cc56cdb191c60db2f0bae796b-Paper.pdf), NeurIPS 2020 [code](https://github.com/nesl/Explainability-Study)

[Asymmetric Shapley values: incorporating causal knowledge into model-agnostic explainability](https://papers.nips.cc/paper/2020/file/0d770c496aa3da6d2c3f2bd19e7b9d6b-Paper.pdf), NeurIPS 2020

[Parameterized Explainer for Graph Neural Network](https://proceedings.neurips.cc/paper/2020/file/e37b08dd3015330dcbb5d6663667b8b8-Paper.pdf), NeurIPS 2020

[PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks](https://proceedings.neurips.cc/paper/2020/file/8fb134f258b1f7865a6ab2d935a897c9-Paper.pdf), NeurIPS 2020

[Visualizing the Impact of Feature Attribution Baselines](https://distill.pub/2020/attribution-baselines/), Distill 2020

[There and Back Again: Revisiting Backpropagation Saliency Methods](https://openaccess.thecvf.com/content_CVPR_2020/papers/Rebuffi_There_and_Back_Again_Revisiting_Backpropagation_Saliency_Methods_CVPR_2020_paper.pdf), CVPR 2020

[Towards Visually Explaining Variational Autoencoders](https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Towards_Visually_Explaining_Variational_Autoencoders_CVPR_2020_paper.pdf), CVPR 2020

Blur integrated gradient: [Attribution in Scale and Space](https://arxiv.org/pdf/2004.03383.pdf), CVPR 2020

[Understanding Integrated Gradients with SmoothTaylor for Deep Neural Network Attribution](https://arxiv.org/pdf/2004.10484.pdf) arxiv preprint 2020

[Visualizing Deep Networks by Optimizing with Integrated Gradients](https://aaai.org/Papers/AAAI/2020GB/AAAI-QiZ.4029.pdf), AAAI 2020

[Relative Attributing Propagation: Interpreting the Comparative Contributions of Individual Units in Deep Neural Networks](https://arxiv.org/pdf/1904.00605.pdf), AAAI 2020

[LS-Tree: Model Interpretation When the Data Are Linguistic](https://ojs.aaai.org/index.php/AAAI/article/view/5749/5605), AAAI 2020, [slides](https://www.jianbochen.me/files/slides_ls_tree.pdf)

[Robust and Stable Black Box Explanations](https://proceedings.icml.cc/static/paper_files/icml/2020/5945-Paper.pdf), ICML 2020

[Concise Explanations of Neural Networks using Adversarial Training](https://arxiv.org/pdf/1810.06583.pdf), ICML 2020

[TOWARDS HIERARCHICAL IMPORTANCE ATTRIBUTION: EXPLAINING COMPOSITIONAL SEMANTICS FOR NEURAL SEQUENCE MODELS](https://iclr.cc/virtual_2020/poster_BkxRRkSKwr.html), ICLR 2020

[Feature relevance quantification in explainable AI: A causal problem relevance quantification in explainable AI: A causal problem](http://proceedings.mlr.press/v108/janzing20a/janzing20a.pdf), AISTATS 2020

[Bias also matters: Bias attribution for deep neural network explanation](http://proceedings.mlr.press/v97/wang19p.html), ICML 2019

[Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation](https://arxiv.org/pdf/1902.00407.pdf), ICML 2019

[On the Connection Between Adversarial Robustness and Saliency Map Interpretability](https://arxiv.org/pdf/1905.04172.pdf), ICML 2019

[Explaining Deep Neural Networks with a Polynomial Time Algorithm for Shapley Value Approximation](http://proceedings.mlr.press/v97/ancona19a/ancona19a.pdf), ICML 2019

[Explainability Techniques for Graph Convolutional Networks](https://arxiv.org/abs/1905.13686), ICML Workshop 2019

FullGrad, [Full-Gradient Representation for Neural Network Visualization](https://papers.nips.cc/paper/8666-full-gradient-representation-for-neural-network-visualization.pdf), NeurIPS 2019

[Towards Automatic Concept-based Explanations](https://papers.nips.cc/paper/2019/file/77d2afcb31f6493e350fca61764efb9a-Paper.pdf), NeurIPS 2019

[GNNExplainer: Generating Explanations for Graph Neural Networks](https://proceedings.neurips.cc/paper/2019/hash/d80b7040b773199015de6d3b4293c8ff-Abstract.html), NeurIPS 2019

[On the (In)fidelity and Sensitivity for Explanations](https://arxiv.org/abs/1901.09392v4), NeurIPS 2019

[Interpretation of Neural Networks is Fragile](https://arxiv.org/abs/1710.10547), AAAI 2019

[XRAI: Better Attributions Through Regions](http://openaccess.thecvf.com/content_ICCV_2019/papers/Kapishnikov_XRAI_Better_Attributions_Through_Regions_ICCV_2019_paper.pdf), ICCV 2019

[Understanding Deep Networks via Extremal Perturbations and Smooth Masks](https://arxiv.org/pdf/1910.08485.pdf), ICCV 2019

[L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data](https://arxiv.org/pdf/1808.02610.pdf), ICLR 2019

[Interpretable and Fine-Grained Visual Explanations for Convolutional Neural Networks](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wagner_Interpretable_and_Fine-Grained_Visual_Explanations_for_Convolutional_Neural_Networks_CVPR_2019_paper.pdf), CVPR 2019

[Explainability Methods for Graph Convolutional Neural Networks](https://openaccess.thecvf.com/content_CVPR_2019/papers/Pope_Explainability_Methods_for_Graph_Convolutional_Neural_Networks_CVPR_2019_paper.pdf), CVPR 2019

[This Looks Like That: Deep Learning for Interpretable Image Recognition](http://papers.nips.cc/paper/9095-this-looks-like-that-deep-learning-for-interpretable-image-recognition.pdf), NeurIPS 2019

[“Why Should You Trust My Explanation?” Understanding Uncertainty in LIME Explanations](https://arxiv.org/pdf/1904.12991.pdf), ICML 2019

[Gradient-Based Vs. Propagation-Based Explanations: An Axiomatic Comparison](https://link.springer.com/chapter/10.1007/978-3-030-28954-6_13), In book: Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, pp.253-265, Springer 2019

[The Many Shapley Values for Model Explanation](https://arxiv.org/pdf/1908.08474.pdf), arxiv preprint 2019

[Explaining the Explainer: A First Theoretical Analysis of LIME](http://arxiv.org/abs/2001.03447), arxiv preprint 2020

VarGard,[Local Explanation Methods for Deep Neural Networks Lack Sensitivity to Parameter Values](https://arxiv.org/abs/1810.03307), ICLR 2018 workshop


NoiseTunnel, [Sanity checks for saliency maps](http://arxiv.org/abs/1810.03292), NeurIPS 2018

[Towards Robust Interpretability with Self-Explaining Neural Networks](https://papers.nips.cc/paper/8003-towards-robust-interpretability-with-self-explaining-neural-networks), NeurIPS 2018

[Model Agnostic Supervised Local Explanations](https://arxiv.org/abs/1807.02910), NeurIPS 2018

Integrated Gradients, [Did the Model Understand the Question?](http://arxiv.org/abs/1805.05492), ACL 2018

Neuron Integrated Gradients: [Computationally Efficient Measures of Internal Neuron Importance](http://arxiv.org/abs/1807.09946) , preprint 2018

TCAV: [Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)](https://arxiv.org/abs/1711.11279), ICML 2018

[A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations](http://proceedings.mlr.press/v80/nie18a/nie18a.pdf), ICML 2018

L2X: [Learning to Explain: An Information-Theoretic Perspective on Model Interpretation](https://arxiv.org/pdf/1802.07814.pdf), ICML 2018 [code](https://github.com/Jianbo-Lab/L2X)

[Noise-adding Methods of Saliency Map as Series of Higher Order Partial Derivative](https://arxiv.org/pdf/1806.03000.pdf), ICML 2018 workshop

InternalInfluence, [Influence-Directed Explanations for Deep Convolutional Networks](http://arxiv.org/abs/1802.03788), IEEE International Test Conference 2018

[Interpretable Basis Decomposition for Visual Explanation](https://openaccess.thecvf.com/content_ECCV_2018/papers/Antonio_Torralba_Interpretable_Basis_Decomposition_ECCV_2018_paper.pdf), 2018 ECCV

[Grounding Visual Explanations](https://arxiv.org/abs/1807.09685), ECCV 2018

RuleMatrix: [RuleMatrix: Visualizing and Understanding Classifiers with Rules](https://arxiv.org/pdf/1807.06228.pdf), TVCG 2018

Manifold: [Manifold: A Model-Agnostic Framework for Interpretation and Diagnosis of Machine Learning Models](https://arxiv.org/pdf/1808.00196.pdf), TVCG 2018

[Top-down neural attention by excitation backprop](https://link.springer.com/article/10.1007/s11263-017-1059-x), IJCV 2018, (ECCV 2016)

[RISE: Randomized Input Sampling for Explanation of Black-box Models](http://bmvc2018.org/contents/papers/1064.pdf), BMVC 2018

Shap: [A unified approach to interpreting model predictions](http://arxiv.org/abs/1705.07874), NeurIPS 2017

[Real Time Image Saliency for Black Box Classifiers](https://papers.nips.cc/paper/7272-real-time-image-saliency-for-black-box-classifiers.pdf), NeurIPS 2017

[Explaining nonlinear classification decisions with deep Taylor decomposition](https://www.sciencedirect.com/science/article/pii/S0031320316303582), Pattern Recognition 2017

[Interpretable Explanations of Black Boxes by Meaningful Perturbation](https://arxiv.org/pdf/1704.03296.pdf), ICCV 2017

[Hide-and-Seek: Forcing a Network to be Meticulous for Weakly-Supervised Object and Action Localization](https://ieeexplore.ieee.org/abstract/document/8237643), ICCV 2017

[Network Dissection: Quantifying Interpretability of Deep Visual Representations](https://arxiv.org/pdf/1704.05796.pdf), CVPR 2017

DeepLIFT: [Learning important features through propagating activation differences](http://arxiv.org/abs/1704.02685), ICML 2017

Integrated Gradients: [Axiomatic attribution for deep networks](http://arxiv.org/abs/1703.01365), ICML 2017

SmoothGard: [SmoothGrad: removing noise by adding noise](http://arxiv.org/abs/1706.03825), ICML 2017

[Visualizing deep neural network decisions: Prediction difference analysis](https://arxiv.org/pdf/1702.04595.pdf), ICLR 2017

[Visualizing deep neural net- work decisions: Prediction difference analysis](https://arxiv.org/abs/1702.04595), arxiv preprint 2017

Lime: ["Why Should I Trust You?": Explaining the Predictions of Any Classifier](http://arxiv.org/abs/1602.04938), SIGKDD 2016

Grad-CAM: [Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization](http://arxiv.org/abs/1610.02391) IJCV 2016

[Visualizing deep convolutional neural networks using natural pre-images](https://arxiv.org/abs/1512.02017), IJCV 2016

[Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models](https://arxiv.org/pdf/1612.08468.pdf), Arxiv preprint 2016

[Salient deconvolutional networks](https://www.robots.ox.ac.uk/~vedaldi/assets/pubs/mahendran16salient.pdf), ECCV 2016

[Top-down Neural Attention by Excitation Backprop](https://arxiv.org/pdf/1608.00507.pdf), ECCV 2016

LRP: [Layer-wise relevance propagation for neural networks with local renormalization layers](http://arxiv.org/abs/1604.00825), ICANN 2016

Gradient * input: [Not Just a Black Box: Learning Important Features Through Propagating Activation Differences](https://arxiv.org/abs/1605.01713), arxiv preprint 2016

InputXGradient, [Investigating the influence of noise and distractors on the interpretation of neural networks](https://arxiv.org/pdf/1611.07270.pdf), NeurIPS 2016

QII, [Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems](https://ieeexplore.ieee.org/document/7546525), IEEE Symposium on Security and Privacy (SP)

epsilon-LRP, [On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4498753/), PloS one 2015

Perturbation-Based method, [Predicting effects of noncoding variants with deep learning–based sequence model](https://www.ncbi.nlm.nih.gov/pubmed/26301843), nature method 2015

CAM: [Learning Deep Features for Discriminative Localization](http://arxiv.org/abs/1512.04150), CVPR 2015

Guided Backpropagation, [Striving for simplicity: The all convolutional net](http://arxiv.org/abs/1412.6806), ICLR 2015

[Understanding neural networks through deep visualization](https://arxiv.org/abs/1506.06579), arxiv preprint 2015

Back progagation: [Deep inside convolutional networks: Visualising image classification models and saliency maps](http://arxiv.org/abs/1312.6034), ICLR 2014

Deconvnet: [Visualizing and Understanding Convolutional Networks](http://arxiv.org/abs/1311.2901)

Shapley sampling values: [Explaining prediction models and individual predictions with feature contributions](https://dl.acm.org/doi/10.1007/s10115-013-0679-x), ACM Knowledge and Information Systems 2014

[Bounding the Estimation Error of Sampling-based Shapley Value Approximation](https://arxiv.org/pdf/1306.4265.pdf), arxiv preprint 2013

[Permutation importance: a corrected feature importance measure](https://academic.oup.com/bioinformatics/article/26/10/1340/193348), Bioinformatics 2010

[How to explain individual classification decisions](http://arxiv.org/abs/0912.1128), Journal of Machine Learning Research 2010

[An Efficient Explanation of Individual Classifications using Game Theory](https://www.jmlr.org/papers/volume11/strumbelj10a/strumbelj10a.pdf), Journal of Machine Learning Research 2010

[Explaining Classifications for Individual Instances](http://lkm.fri.uni-lj.si/rmarko/papers/RobnikSikonjaKononenko08-TKDE.pdf), TKDE 2008

[Review and comparison of methods to study the contribution of variables in artificial neural network models](https://www.sciencedirect.com/science/article/abs/pii/S0304380002002570), Ecological Modelling 2003
